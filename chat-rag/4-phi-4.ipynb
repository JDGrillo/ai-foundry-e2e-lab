{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "352dafe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Model deployment name: <your-serverless-model-name>\n",
      "üìû Connection string: https://foundrye2elab.services.ai.azure.com/api/projects/banff\n",
      "üîÑ Alternative model from MODEL_DEPLOYMENT_NAME: gpt-4.1-mini\n",
      "‚úÖ Using MODEL_DEPLOYMENT_NAME instead: gpt-4.1-mini\n"
     ]
    }
   ],
   "source": [
    "# Let's check what model deployment is being used\n",
    "print(f\"ü§ñ Model deployment name: {phi4_deployment}\")\n",
    "print(f\"üìû Connection string: {conn_string}\")\n",
    "\n",
    "# Let's try using the same model that worked in other notebooks\n",
    "model_deployment_name = os.getenv(\"MODEL_DEPLOYMENT_NAME\")\n",
    "print(f\"üîÑ Alternative model from MODEL_DEPLOYMENT_NAME: {model_deployment_name}\")\n",
    "\n",
    "# If phi-4 deployment doesn't exist, let's use the working model instead\n",
    "if model_deployment_name:\n",
    "    phi4_deployment = model_deployment_name\n",
    "    print(f\"‚úÖ Using MODEL_DEPLOYMENT_NAME instead: {phi4_deployment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e99a5b8",
   "metadata": {},
   "source": [
    "# üçé Phi-4 Model with AIProjectClient üçè\n",
    "\n",
    "**Phi-4** is a next-generation open model that aims to provide near GPT-4o capabilities at a fraction of the cost, making it ideal for many enterprise or personal use cases. It's especially great for chain-of-thought reasoning and RAG (Retrieval Augmented Generation) scenarios.\n",
    "\n",
    "In this notebook, you'll see how to:\n",
    "1. **Initialize** an `AIProjectClient` for your Azure AI Foundry environment.\n",
    "2. **Chat** with the **Phi-4** model using `azure-ai-inference`.\n",
    "3. **Show** a Health & Fitness example, featuring disclaimers and wellness Q&A.\n",
    "4. **Enjoy** the value proposition of a cheaper alternative to GPT-4 with strong reasoning capabilities. üèãÔ∏è\n",
    "\n",
    "> **Disclaimer**: This is not medical advice. Please consult professionals.\n",
    "\n",
    "## Why Phi-4?\n",
    "Phi-4 is a 14B-parameter model trained on curated data for high reasoning performance.\n",
    "- **Cost-Effective**: Get GPT-4-level performance for many tasks without the GPT-4 price.\n",
    "- **Reasoning & RAG**: Perfect for chain-of-thought reasoning steps and retrieval augmented generation workflows.\n",
    "- **Generous Context Window**: 16K tokens, enabling more context or longer user conversations.\n",
    "\n",
    "<img src=\"./seq-diagrams/4-phi-4.png\" width=\"30%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93357dd",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Below, we'll install and import the necessary libraries:\n",
    "- **azure-ai-projects**: For the `AIProjectClient`.\n",
    "- **azure-ai-inference**: For calling your model, specifically the chat completions.\n",
    "- **azure-identity**: For `DefaultAzureCredential`.\n",
    "\n",
    "Ensure you have a `.env` file with:\n",
    "```bash\n",
    "PROJECT_CONNECTION_STRING=<your-conn-string>\n",
    "SERVERLESS_MODEL_NAME=phi-4\n",
    "```\n",
    "\n",
    "> **Note**: It's recommended to complete the [`3-basic-rag.ipynb`](./3-basic-rag.ipynb) notebook before this one, as it covers important concepts that will be helpful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b5634a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîë Using Tenant ID: ed244546-f48e-4572-a767-d6d2a521a7c5\n",
      "üåê Using browser-based authentication to bypass Azure CLI cache issues...\n",
      "‚úÖ AIProjectClient created successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from azure.identity import InteractiveBrowserCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage, AssistantMessage\n",
    "\n",
    "# Load environment variables\n",
    "notebook_path = Path().absolute()\n",
    "parent_dir = notebook_path.parent\n",
    "load_dotenv(parent_dir / '.env')\n",
    "\n",
    "conn_string = os.getenv(\"PROJECT_CONNECTION_STRING\")\n",
    "phi4_deployment = os.getenv(\"SERVERLESS_MODEL_NAME\", \"phi-4\")\n",
    "tenant_id = os.environ.get(\"TENANT_ID\")\n",
    "\n",
    "print(f\"üîë Using Tenant ID: {tenant_id}\")\n",
    "\n",
    "try:\n",
    "    print(\"üåê Using browser-based authentication to bypass Azure CLI cache issues...\")\n",
    "    \n",
    "    # Use only InteractiveBrowserCredential with the specific tenant\n",
    "    credential = InteractiveBrowserCredential(tenant_id=tenant_id)\n",
    "    \n",
    "    # Create the project client using endpoint (conn_string is actually the endpoint URL)\n",
    "    project_client = AIProjectClient(\n",
    "        endpoint=conn_string,\n",
    "        credential=credential\n",
    "    )\n",
    "    print(\"‚úÖ AIProjectClient created successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Error creating AIProjectClient:\", e)\n",
    "    print(\"üí° Please complete the browser authentication prompt that should appear\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Error creating AIProjectClient:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500d63ef",
   "metadata": {},
   "source": [
    "## 2. Chat with Phi-4 üçè\n",
    "We'll demonstrate a simple conversation using **Phi-4** in a health & fitness context. We'll define a system prompt that clarifies the role of the assistant. Then we'll ask some user queries.\n",
    "\n",
    "> Notice that Phi-4 is well-suited for chain-of-thought reasoning. We'll let it illustrate its reasoning steps for fun.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bcc4772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üó£Ô∏è User: I'm training for a 5K. Any tips on a weekly workout schedule?\n",
      "ü§ñ Phi-4: Great that you're training for a 5K! While I'm not a medical professional, I can offer some general guidance on structuring a weekly workout schedule to help you prepare effectively.\n",
      "\n",
      "Here's a step-by-step reasoning process to build a balanced 5K training week:\n",
      "\n",
      "1. **Include running days:** To build endurance and speed, you need to run several times a week. Beginners often start with 3-4 running days.\n",
      "\n",
      "2. **Vary intensity:** Mix easy runs, speed work (like intervals or tempo runs), and longer runs to improve different aspects of fitness.\n",
      "\n",
      "3. **Incorporate rest or cross-training:** Rest days allow recovery, while cross-training (cycling, swimming, yoga) can enhance fitness without added running strain.\n",
      "\n",
      "4. **Add strength training:** Strengthening muscles can improve running economy and reduce injury risk.\n",
      "\n",
      "Based on this, a sample weekly schedule might look like:\n",
      "\n",
      "- **Monday:** Easy run (20-30 minutes) + strength training (bodyweight exercises or light weights)\n",
      "- **Tuesday:** Rest or cross-training (e.g., swimming or cycling)\n",
      "- **Wednesday:** Interval training (e.g., 4x400m at faster pace with rest intervals)\n",
      "- **Thursday:** Easy run or cross-training\n",
      "- **Friday:** Rest\n",
      "- **Saturday:** Long run (gradually increasing distance, e.g., starting at 3 miles)\n",
      "- **Sunday:** Rest or light cross-training/stretching\n",
      "\n",
      "Make sure to warm up before runs and cool down afterward. Also, listen to your body to avoid overtraining or injury.\n",
      "\n",
      "If you have any health conditions or are new to exercise, consider consulting a healthcare professional or running coach to tailor a plan to your needs.\n",
      "\n",
      "Would you like help designing specific workouts or pacing strategies?\n"
     ]
    }
   ],
   "source": [
    "def chat_with_phi4(user_question, chain_of_thought=False):\n",
    "    \"\"\"Send a chat request to the Phi-4 model with optional chain-of-thought.\"\"\"\n",
    "    # We'll define a system message with disclaimers:\n",
    "    system_prompt = (\n",
    "        \"You are a Phi-4 AI assistant, focusing on health and fitness.\\n\"\n",
    "        \"Remind users that you are not a medical professional, but can provide general info.\\n\"\n",
    "    )\n",
    "\n",
    "    # We can optionally instruct the model to show chain-of-thought. (Use carefully in production.)\n",
    "    if chain_of_thought:\n",
    "        system_prompt += \"Please show your step-by-step reasoning in your answer.\\n\"\n",
    "\n",
    "    # We create messages for system + user.\n",
    "    system_msg = SystemMessage(content=system_prompt)\n",
    "    user_msg = UserMessage(content=user_question)\n",
    "\n",
    "    with project_client.get_openai_client(api_version=\"2024-10-21\") as chat_client:\n",
    "        response = chat_client.chat.completions.create(\n",
    "            model=phi4_deployment,\n",
    "            messages=[system_msg, user_msg],\n",
    "            temperature=0.8,  # a bit creative\n",
    "            top_p=0.9,\n",
    "            max_tokens=400,\n",
    "        )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example usage:\n",
    "question = \"I'm training for a 5K. Any tips on a weekly workout schedule?\"\n",
    "answer = chat_with_phi4(question, chain_of_thought=True)\n",
    "print(\"üó£Ô∏è User:\", question)\n",
    "print(\"ü§ñ Phi-4:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc68c40d",
   "metadata": {},
   "source": [
    "## 3. RAG-like Example (Stub)\n",
    "Phi-4 also excels in retrieval augmented generation scenarios, where you provide external context and let the model reason over it. Below is a **stub** example showing how you'd pass retrieved text as context.\n",
    "\n",
    "> In a real scenario, you'd embed & search for relevant passages, then feed them into the user/system message.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "419ea578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üó£Ô∏è User: How often should I run weekly to prepare for a 5K?\n",
      "ü§ñ Phi-4 (RAG): To prepare for a 5K, it's recommended to run about 3 times per week. This allows you to build endurance and speed while minimizing the risk of injury. Be sure to mix your runs with cross-training activities like cycling or swimming to improve overall fitness. Also, include rest days or active recovery days to help your muscles repair and prevent burnout. Would you like a sample weekly running plan?\n"
     ]
    }
   ],
   "source": [
    "def chat_with_phi4_rag(user_question, retrieved_doc):\n",
    "    \"\"\"Simulate an RAG flow by appending retrieved context to the system prompt.\"\"\"\n",
    "    system_prompt = (\n",
    "        \"You are Phi-4, helpful fitness AI.\\n\"\n",
    "        \"We have some context from the user's knowledge base: \\n\"\n",
    "        f\"{retrieved_doc}\\n\"\n",
    "        \"Please use this context to help your answer. If the context doesn't help, say so.\\n\"\n",
    "    )\n",
    "\n",
    "    system_msg = SystemMessage(content=system_prompt)\n",
    "    user_msg = UserMessage(content=user_question)\n",
    "\n",
    "    with project_client.get_openai_client(api_version=\"2024-10-21\") as chat_client:\n",
    "        response = chat_client.chat.completions.create(\n",
    "            model=phi4_deployment,\n",
    "            messages=[system_msg, user_msg],\n",
    "            temperature=0.3,\n",
    "            max_tokens=300,\n",
    "        )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Let's define a dummy doc snippet:\n",
    "doc_snippet = \"Recommended to run 3 times per week and mix with cross-training.\\n\" \\\n",
    "              \"Include rest days or active recovery days for muscle repair.\"\n",
    "\n",
    "user_q = \"How often should I run weekly to prepare for a 5K?\"\n",
    "rag_answer = chat_with_phi4_rag(user_q, doc_snippet)\n",
    "print(\"üó£Ô∏è User:\", user_q)\n",
    "print(\"ü§ñ Phi-4 (RAG):\", rag_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a33a375",
   "metadata": {},
   "source": [
    "## 4. Wrap-Up & Best Practices\n",
    "1. **Chain-of-Thought**: Great for debugging or certain QA tasks, but be mindful about revealing chain-of-thought to end users.\n",
    "2. **RAG**: Use `azure-ai-inference` with retrieval results to ground your answers.\n",
    "3. **OpenTelemetry**: Optionally integrate `opentelemetry-sdk` and `azure-core-tracing-opentelemetry` for full observability.\n",
    "4. **Evaluate**: Use `azure-ai-evaluation` to measure your model‚Äôs performance.\n",
    "5. **Cost & Performance**: Phi-4 aims to provide near GPT-4 performance at lower cost. Evaluate for your domain needs.\n",
    "\n",
    "## üéâ Congratulations!\n",
    "You've seen how to:\n",
    "- Use **Phi-4** with `AIProjectClient` and `azure-ai-inference`.\n",
    "- Create a **chat** flow with chain-of-thought.\n",
    "- Stub a **RAG** scenario.\n",
    "\n",
    "> Happy hacking! üèãÔ∏è\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
